{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeeba911",
   "metadata": {},
   "source": [
    "**Ensemble learning** is the technique of combining predictions from multiple machine learning models to produce a more accurate and robust prediction than any single model. **Random Forest** is a popular ensemble method that builds many decision trees and merges their outputs.\n",
    "\n",
    "The two main types of ensemble learning are **bagging** and **boosting**. Bagging, like in a Random Forest, builds independent models in parallel to reduce variance. Boosting, on the other hand, builds models sequentially, with each new model focusing on the mistakes of the previous one to reduce bias.\n",
    "\n",
    "---\n",
    "## 1. Ensemble Learning and Random Forest ðŸŒ³\n",
    "\n",
    "### Ensemble Learning: The Wisdom of the Crowd\n",
    "The core idea behind ensemble learning is the \"wisdom of the crowd.\" Instead of relying on a single expert (one model), you ask a committee of diverse experts (multiple models) and aggregate their opinions. The combined decision is typically better than any individual expert's opinion.\n",
    "\n",
    "This works because the strengths of one model can compensate for the weaknesses of another, leading to:\n",
    "* **Higher Accuracy**: The combined model is more powerful.\n",
    "* **Better Generalization**: The model is less likely to overfit to the training data.\n",
    "\n",
    "### Random Forest\n",
    "A **Random Forest** is a specific ensemble method that uses decision trees as its base models. It creates a \"forest\" of many decision trees and combines their predictions for a final result.\n",
    "\n",
    "Hereâ€™s how it works:\n",
    "1.  **Bootstrap Sampling**: It creates multiple random samples of the training data *with replacement*. This means some data points may be selected multiple times in one sample, while others may not be selected at all. Each tree is trained on a different sample.\n",
    "2.  **Feature Randomness**: When splitting a node, each tree only considers a random subset of the total features. This ensures that the trees are diverse and don't all rely on the same few powerful predictors.\n",
    "3.  **Voting/Averaging**: Once all the trees are trained, a new data point is passed through each tree.\n",
    "    * For **classification**, the final prediction is the class that gets the most votes.\n",
    "    * For **regression**, the final prediction is the average of all the individual tree predictions.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 2. Bagging vs. Boosting\n",
    "\n",
    "Bagging and boosting are two different strategies for creating an ensemble.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "Bagging focuses on creating multiple independent models and averaging their predictions to reduce variance and prevent overfitting.\n",
    "\n",
    "* **How it works**: Models are trained **in parallel** on different bootstrap samples of the data.\n",
    "* **Goal**: To create a robust model that is less sensitive to the specific training data it was built on. It reduces **variance**.\n",
    "* **Prime Example**: **Random Forest** is the most famous application of bagging.\n",
    "\n",
    "**Analogy**: Imagine you give the same complex problem to several different students (models). You let them work independently and then average their answers. The final averaged answer is likely to be more reliable and less extreme than any single student's answer.\n",
    "\n",
    "### Boosting\n",
    "Boosting focuses on building models **sequentially**, where each model learns from the errors of the one that came before it.\n",
    "\n",
    "* **How it works**:\n",
    "    1.  A simple model (a \"weak learner,\" like a small decision tree) is trained on the data.\n",
    "    2.  The model's mistakes (misclassified data points) are identified.\n",
    "    3.  The next model is trained with a higher weight given to these difficult, misclassified points, forcing it to focus on fixing the previous model's errors.\n",
    "    4.  This process is repeated, with each new model correcting its predecessor.\n",
    "* **Goal**: To combine many weak models into a single, highly accurate \"strong learner.\" It reduces **bias**.\n",
    "* **Popular Algorithms**: AdaBoost, Gradient Boosting (GBM), XGBoost, and LightGBM.\n",
    "\n",
    "**Analogy**: Imagine a team of students working on a problem one after another. The first student tries to solve it. The second student looks at the first student's mistakes and focuses specifically on fixing them. The third student then focuses on the remaining mistakes, and so on. The final result is a highly polished and accurate solution.\n",
    "\n",
    "### Summary of Differences\n",
    "\n",
    "| Feature | **Bagging** | **Boosting** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Model Training** | Parallel (Independent) | Sequential (Dependent) |\n",
    "| **Primary Goal** | Reduce Variance (Avoid Overfitting) | Reduce Bias (Improve Accuracy) |\n",
    "| **Data Weighting**| All data points are weighted equally | Misclassified points are given more weight |\n",
    "| **Model Type** | Uses complex base models (e.g., full decision trees) | Uses simple \"weak\" base models (e.g., small decision trees) |\n",
    "| **Example** | Random Forest | XGBoost, AdaBoost, Gradient Boosting |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1767db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyankateshgoski\\AppData\\Local\\Temp\\9\\ipykernel_17696\\2557609286.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['IncomeClass'] = df['IncomeClass'].replace({'<=50K': 0, '>50K': 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.7970355398349335\n",
      "Confusion Matrix:\n",
      " [[3833  540]\n",
      " [ 665  899]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.877     0.864      4373\n",
      "           1      0.625     0.575     0.599      1564\n",
      "\n",
      "    accuracy                          0.797      5937\n",
      "   macro avg      0.738     0.726     0.731      5937\n",
      "weighted avg      0.792     0.797     0.794      5937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load & clean\n",
    "df = pd.read_csv('04+-+decisiontreeAdultIncome.csv', skipinitialspace=True)\n",
    "df.columns = [c.strip().replace(' ', '_') for c in df.columns]\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "# Target -> binary\n",
    "df['IncomeClass'] = df['IncomeClass'].replace({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "# 2. Split features / target\n",
    "X = df.drop('IncomeClass', axis=1)\n",
    "y = df['IncomeClass']\n",
    "\n",
    "# 3. One-hot encode categoricals\n",
    "cat_cols = X.select_dtypes(include='object').columns\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# 4. Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Baseline tree\n",
    "tree = RandomForestClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Baseline Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669a65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
